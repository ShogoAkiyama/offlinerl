# BCQ: Off-Policy Deep Reinforcement LEarning without Exploration



## Abstract

オフライン強化学習は、固定されたデータの中での学習である。本研究ではオフライン学習の問題設定に置いて、DQNやDDPGで学習する際に生じる$Extrapolation ~ Error$問題を説明するとともに、その問題を解決するためのアルゴリズムであるBCQを紹介する。



## 1. Introduction

固定されたデータセットからの学習としては模倣学習がある。模倣学習は最適でない方策からのデータがある場合に上手くいかないことや、学習後に環境から追加学習が必要であることがある。その点オフライン強化学習は**データの質の制約はない**、つまり最適でない方策からのデータがある場合でも、最適な方策を探索することができる。

ここでオフライン強化学習と関係が深い、Off-Policy強化学習を説明する。DQNなどのOff-Policy強化学習は挙動方策で行動し、環境からサンプルしたデータをリプレイメモリ$D$に貯め、そのデータセットからの学習用データで学習方策を更新する。ここでの問題として、**学習方策とリプレイメモリ内のデータをサンプルした時の方策の相関が低い**（学習方策が本当にほしいデータじゃないサンプルが多い）場合、学習効率が著しく悪くなってしまう。オンライン強化学習の場合には、挙動方策が学習方策に置き換えられ、その方策からサンプルしたデータでリプレイメモリが更新されるため、相関は高くなっていくが、オフライン学習でた追加でのサンプルがないため非常に大きな問題である。

（相関が低い例: 例えば状態$s$で行動$a_1$を取ったデータしかリプレイメモリ上に無く、行動$a_2$でのデータがいつまで経っても来ない場合、学習方策は上手く学習されない。一方でリプレイメモリのサイズを小さくすると、サンプル効率が下がるためoff-policyの利点が無くなる。）

このoff-policy学習で学習方策とリプレイメモリの相関が低いことにより生じる学習誤差の問題を$Extrapolation ~ Error$と呼ぶ。



オフライン強化学習の話としては、２つに別れる。

1. **学習方策に制約**を与えることで、リプレイメモリ内での方策となるべく近くする。つまり、なるべくデータセット内に存在する状態行動対を選択する。
2. データセット内に**存在しない**状態行動対の報酬には**罰則**を与える。



BCQのアプローチとしては1である。簡単に説明すると、データセット内の方策を$Behavior~Cloning$し、その近辺の行動を選択する。



## 3. Extrapolation Error

オフライン強化学習で起きる$Extrapolation~Error$とはなにか。上で軽く説明したが、**学習方策とリプレイメモリ内のデータサンプル時の方策の相関が低い**場合に起こる推定誤差である。具体的な例を下の図に示す。オンライン強化学習は、環境と相互作用により行動（左）→価値修正→行動（右）→価値修正となる。

一方でオフライン強化学習では、右行動のデータが無い場合などがある。これにより、$Q$値は修正されず、右行動への過大評価が起きる。この$\epsilon$の値が$Extrapolation~Error$である。強化学習では、価値関数は時間を遡って更新されるため、この$\epsilon$が他の価値関数に伝播されていく。

![Screenshot at 2020-12-18 19-07-08](/home/shogo/Desktop/offlinerl/thesis_summarize/image/BCQ/Screenshot at 2020-12-18 19-07-08.png)



## 4. Batch-Constrained Reinforcement Learning

